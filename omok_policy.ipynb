{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "gridSize = 7\n",
    "nbActions = gridSize * gridSize\n",
    "nbStates = gridSize * gridSize\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "INPUT = nbStates\n",
    "OUTPUT = nbActions\n",
    "DISCOUNT = 0.99\n",
    "\n",
    "winReward = 1\n",
    "\n",
    "STONE_NONE = 0\n",
    "STONE_PLAYER1 = 1\n",
    "STONE_PLAYER2 = -1\n",
    "STONE_MAX = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hiddenSize = 100\n",
    "maxMemory = 500\n",
    "batchSize = 50\n",
    "epoch = 100\n",
    "epsilonStart = 1\n",
    "epsilonDiscount = 0.999\n",
    "epsilonMinimumValue = 0.1\n",
    "discount = 0.9\n",
    "learningRate = 0.0001\n",
    "winReward = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Omok:\n",
    "    # --------------------------------\n",
    "    # 초기화\n",
    "    # --------------------------------\n",
    "    def __init__(self, gridSize):\n",
    "        self.gridSize = gridSize\n",
    "        self.nbStates = self.gridSize * self.gridSize\n",
    "        self.state = np.zeros(self.nbStates, dtype=np.uint8)\n",
    "        self.curPlayer = 1\n",
    "        \n",
    "    # --------------------------------\n",
    "    # 리셋\n",
    "    # --------------------------------\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.nbStates, dtype=np.uint8)\n",
    "        \n",
    "    # --------------------------------\n",
    "    # 현재 상태 구함\n",
    "    # --------------------------------\n",
    "    def getState(self):\n",
    "        return np.reshape(self.state, (1, self.nbStates))\n",
    "    \n",
    "    # --------------------------------\n",
    "    # render\n",
    "    # --------------------------------\n",
    "    def render(self):\n",
    "        state = np.reshape(self.state,(self.gridSize,self.gridSize))\n",
    "        print(state)\n",
    "        return state\n",
    "    \n",
    "    # --------------------------------\n",
    "    # 매칭 검사\n",
    "    # 다섯개 붙어 있으면 true, 아님 false\n",
    "    # --------------------------------\n",
    "    def CheckMatch(self, player):\n",
    "        for y in range(self.gridSize):\n",
    "            for x in range(self.gridSize):\n",
    "\n",
    "                # --------------------------------\n",
    "                # 오른쪽 검사\n",
    "                # --------------------------------\n",
    "                match = 0\n",
    "\n",
    "                for i in range(STONE_MAX):\n",
    "                    if (x + i >= self.gridSize):\n",
    "                        break\n",
    "\n",
    "                    if (self.state[y * self.gridSize + x + i] == player):\n",
    "                        match += 1\n",
    "                    else:\n",
    "                        break;\n",
    "\n",
    "                    if (match >= STONE_MAX):\n",
    "                        return True\n",
    "\n",
    "                # --------------------------------\n",
    "                # 아래쪽 검사\n",
    "                # --------------------------------\n",
    "                match = 0\n",
    "\n",
    "                for i in range(STONE_MAX):\n",
    "                    if (y + i >= self.gridSize):\n",
    "                        break\n",
    "\n",
    "                    if (self.state[(y + i) * self.gridSize + x] == player):\n",
    "                        match += 1\n",
    "                    else:\n",
    "                        break;\n",
    "\n",
    "                    if (match >= STONE_MAX):\n",
    "                        return True\n",
    "\n",
    "                # --------------------------------\n",
    "                # 오른쪽 대각선 검사\n",
    "                # --------------------------------\n",
    "                match = 0\n",
    "\n",
    "                for i in range(STONE_MAX):\n",
    "                    if ((x + i >= self.gridSize) or (y + i >= self.gridSize)):\n",
    "                        break\n",
    "\n",
    "                    if (self.state[(y + i) * self.gridSize + x + i] == player):\n",
    "                        match += 1\n",
    "                    else:\n",
    "                        break;\n",
    "\n",
    "                    if (match >= STONE_MAX):\n",
    "                        return True\n",
    "\n",
    "                # --------------------------------\n",
    "                # 왼쪽 대각선 검사\n",
    "                # --------------------------------\n",
    "                match = 0\n",
    "\n",
    "                for i in range(STONE_MAX):\n",
    "                    if ((x - i < 0) or (y + i >= self.gridSize)):\n",
    "                        break\n",
    "\n",
    "                    if (self.state[(y + i) * self.gridSize + x - i] == player):\n",
    "                        match += 1\n",
    "                    else:\n",
    "                        break;\n",
    "\n",
    "                    if (match >= STONE_MAX):\n",
    "                        return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    # --------------------------------\n",
    "    # 게임오버 검사\n",
    "    # 리워드를 리턴\n",
    "    # 이긴 player : 1, 진 player  : -1, 무승부 : 0\n",
    "    # --------------------------------\n",
    "    def getReward(self, player):\n",
    "        if (self.CheckMatch(STONE_PLAYER1) == True):\n",
    "            if (player == STONE_PLAYER1):\n",
    "                return winReward\n",
    "            else:\n",
    "                return -winReward\n",
    "        elif (self.CheckMatch(STONE_PLAYER2) == True):\n",
    "            if (player == STONE_PLAYER1):\n",
    "                return -winReward\n",
    "            else:\n",
    "                return winReward\n",
    "        else:\n",
    "            for i in range(self.nbStates):\n",
    "                if (self.state[i] == STONE_NONE):\n",
    "                    return 0\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    # --------------------------------\n",
    "    # action취해서 state update\n",
    "    # player : 다음 player로 전환\n",
    "    # --------------------------------\n",
    "    def getNextState(self, state, player, action):\n",
    "        state[action] = player;\n",
    "        self.state = state\n",
    "        player = -player\n",
    "        \n",
    "        return self.state, player\n",
    "    \n",
    "    \n",
    "    # --------------------------------\n",
    "    # curPlayer에 맞는 state\n",
    "    # --------------------------------\n",
    "    def getCanonicalForm(self, state, curPlayer):\n",
    "        return state * curPlayer\n",
    "    \n",
    "    # --------------------------------\n",
    "    # 한 에피소드 동안 실행(게임이 끝날 때까지)\n",
    "    # --------------------------------\n",
    "    def step(self, sess):\n",
    "        trainExamples = []\n",
    "        self.curPlayer = 1\n",
    "        \n",
    "        #episodeStep = 0 # array가 너무 깊어지면 안돌아가서.. 100번안에 episode가 안끝나면 그냥 그대로\n",
    "        \n",
    "        while True:\n",
    "            #episodeStep+=1\n",
    "            \n",
    "            canonicalBoard = self.getCanonicalForm(self.state,self.curPlayer)\n",
    "            pi = sess.run(output_layer, feed_dict={X: np.reshape(canonicalBoard,[1,nbStates]), keep_prob: 1.0})\n",
    "\n",
    "            trainExamples.append([self.state, self.curPlayer, pi])\n",
    "            action = np.random.choice(len(pi[0]), p=pi[0])\n",
    "            \n",
    "            self.state, self.curPlayer = self.getNextState(self.state, self.curPlayer, action)\n",
    "\n",
    "            r = self.getReward(self.curPlayer)\n",
    "            \n",
    "            if r!=0:\n",
    "                return [(x[0],x[2],r*((-1)**(x[1]!=self.curPlayer))) for x in trainExamples]\n",
    "            \n",
    "                # 전체 에피소드 (한 수 한 수....)\n",
    "                # 각 수마다 state, policy, reward\n",
    "                # 마지막에 이긴 player가 둔 수에는 모두 r=1, 진 player가 둔 수는 모두 r=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, nbStates]) # state\n",
    "x_image = tf.reshape(X, [-1, gridSize, gridSize, 1])\n",
    "\n",
    "Y = tf.placeholder('float', [None, nbStates]) # action\n",
    "adv = tf.placeholder('float') # reward\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv1 = weight_variable([3, 3, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([2 * 2 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 2*2*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, nbActions])\n",
    "b_fc2 = bias_variable([nbActions])\n",
    "\n",
    "output_layer=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "log_p = Y * tf.log(output_layer)\n",
    "log_lik = log_p * adv\n",
    "loss = tf.reduce_mean(tf.reduce_sum(-log_lik, axis=1))\n",
    "train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    '''Discounted reward를 구하기 위한 함수\n",
    "    \n",
    "    Args:\n",
    "         r(np.array): reward 값이 저장된 array\n",
    "    \n",
    "    Returns:\n",
    "        discounted_r(np.array): Discounted 된 reward가 저장된 array\n",
    "    '''\n",
    "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(r))):\n",
    "        running_add = running_add * DISCOUNT + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    game = Omok(gridSize)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(100):\n",
    "        trainExamples = []\n",
    "        trainExamples.append(game.step(sess))\n",
    "        \n",
    "        rewards = discount_rewards(trainExamples[:,2])\n",
    "        \n",
    "        cost, _ = sess.run([cost,optimizer], feed_dict={X: trainExamples[:,0], Y: trainExamples[:,1], adv : rewards})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
